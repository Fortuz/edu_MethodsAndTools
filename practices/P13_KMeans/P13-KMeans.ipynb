{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/logo.png\">\n",
    "\n",
    "Made by **Balázs Nagy**\n",
    "\n",
    "[<img src=\"../assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_MethodsAndTools/blob/main/practices/P13_KMeans/P13-KMeans.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means clastering\n",
    "\n",
    "The main groups of machine learning algorithms are illustrated in the figure below, one of the main groups being unsupervised learning algorithms.\n",
    "\n",
    "<img src=\"Pics/L11_groups.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous labs, we have looked in detail at Supervised Learning and its cases, classification and regression. In this practice, we will focus on Unsupervised Learning and clustering within it. \n",
    "\n",
    "When might unsupervised learning be necessary?\n",
    "\n",
    "For example, in cases where we don't know the output but we are looking for some sort of pattern in our data. Market participant analysis, decomposition or relationship network analysis.\n",
    "\n",
    "<img src=\"Pics/L11_example.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim may be to identify small cohesive groups, also known as clusters. One possible solution is to use the K-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Data load\n",
    "\n",
    "Our initial data will be 2D points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "\n",
    "data = loadmat(\"Lab11data.mat\")\n",
    "X = data[\"X\"]\n",
    "plt.plot(X[:,0],X[:,1], 'yo')\n",
    "print('X shape:',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, we would say that the data can be classified into 3 clusters according to the structure that can be discovered in the data set. In terms of how the K-Means algorithm works, we need to determine the number of possible clusters. So let us make this 3. \n",
    "\n",
    "The initial cluster midpoints can be defined either explicitly or randomly.\n",
    "Furthermore, they can be points chosen from the sample or even starting points not included in the sample set. \n",
    "\n",
    "What should be aware that the number K (as number of clusters) should be less than the sample number.\n",
    "\n",
    "In our palette we have predefined the coordinates of the 3 starting cluster centres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3                                                           # number of clusters\n",
    "init_centroids = np.array([[3,3],[6,2],[8,5]])                  \n",
    "print('initial claster centroids:\\n',init_centroids)\n",
    "\n",
    "plt.plot(X[:,0],X[:,1], 'yo')\n",
    "plt.plot(init_centroids[:,0],init_centroids[:,1], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to decide for each point in the sample set which cluster it belongs to, i.e. which cluster centre it is closest to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Finding the nearest centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the following function that assigns to each point the index of the nearest cluster centre. \n",
    "\n",
    "The distance between two points can be calculated using the Pythagorean theorem. <br>\n",
    "Let the two points be $P_1(x_1, y_1)$ és $P_2(x_2, y_2)$. \n",
    "\n",
    "The distance can be calculated as follows: <br>\n",
    "\n",
    "$d = \\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$ <br>\n",
    "\n",
    "For the purpose of comparison, the root can be omitted and we can calculate with $d^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClosestCentroids(X,centroids):    \n",
    "    K = centroids.shape[0]\n",
    "    idx = np.zeros((X.shape[0],1))\n",
    "################################################    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "################################################    \n",
    "    return idx\n",
    "\n",
    "idx = findClosestCentroids(X,init_centroids)\n",
    "print('Closest centroids for the first 3 examples (expected: 0-2-1):\\n',idx[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Calculate the centroids\n",
    "\n",
    "Once we have classified all points into a cluster, we need to review the new cluster centres. We need to recalculate the centroid of the clusters taking into account the points that were clustered. This will step the cluster midpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCentroids(X,idx,K):\n",
    "    \n",
    "    m,n = X.shape\n",
    "    centroids = np.zeros((K,n))\n",
    "    cent = []\n",
    "################################################    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################    \n",
    "    return centroids\n",
    "\n",
    "centroids = computeCentroids(X,idx,K)\n",
    "print(\"\"\"Centroids computed after initial finding of closest centroids:\n",
    "(expected values):\n",
    "[[2.42830111 3.15792418]\n",
    " [5.81350331 2.63365645]\n",
    " [7.11938687 3.6166844 ]]\n",
    " Computed:\n",
    " \"\"\", centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: K-Means clastering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of the K-means algorithm (clustering of points, stepping of centres) are then run until a stopping condition is reached. This condition can be that the midpoints no longer change, or the change stays within a specified small interval, or the algorithm is run up to a prescribed iteration. In this example, we have chosen the latter solution and will run the algorithm for 5 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotkMeans(idx,num_CL,C_H,it):\n",
    "    colors = ('b','g','r','c','m','y','k')\n",
    "    \n",
    "    plt.figure\n",
    "    for i in range(num_CL):\n",
    "        CL_i = X[np.where(idx == i)[0],:]\n",
    "        plt.plot(CL_i[:,0],CL_i[:,1], colors[i]+'o',)\n",
    "        plt.plot(C_H[i,0,0:it+2],C_H[i,1,0:it+2],'kx-')\n",
    "\n",
    "    plt.show()    \n",
    "    return 0\n",
    "\n",
    "def runkMeans(X, init_cents, max_iters, plotProgress= False):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    k = init_cents.shape[0]\n",
    "    idx = np.zeros((m))\n",
    "    centroids = init_cents\n",
    "    cent_hist = np.zeros((init_cents.shape[0],init_cents.shape[1],max_iters+1))\n",
    "    cent_hist[:,:,0] = init_cents\n",
    "    \n",
    "    # K-Means:\n",
    "    for i in range(max_iters):\n",
    "        print('Running the {} iteration of {}'.format(i+1,max_iters))\n",
    "        ################################################ \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "        ################################################ \n",
    "        if plotProgress:\n",
    "            plotkMeans(idx, init_cents.shape[0], cent_hist,i)\n",
    "    \n",
    "    return centroids, idx\n",
    "\n",
    "K = 5\n",
    "max_iters = 5\n",
    "init_cents = np.array([[3,3],[6,2],[8,5]])\n",
    "\n",
    "centroids, idx = runkMeans(X,init_cents,max_iters,True)\n",
    "print(\"\"\"Centroids computed by the algorithm:\n",
    "Expected:\n",
    "[[1.95399466 5.02557006]\n",
    " [3.12663743 1.1121712 ]\n",
    " [6.12919526 3.01606258]]\n",
    "Computed:\n",
    "\"\"\",centroids)\n",
    "\n",
    "print('Centroids movements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how our initial cluster centres migrated to their final location over the 5 iterations, and how the sample classification of the dataset changed accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Problems\n",
    "\n",
    "Depending on the choice of cluster centers (especially in the case of random initialization), the algorithm will arrive at different optimal positions. Furthermore, it can easily get stuck in a local optimum if poorly distributed cluster centres are chosen.\n",
    "\n",
    "<img src=\"Pics/L11_Localopt.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important question is how to choose the value of K (number of clusters).\n",
    "\n",
    "This is mostly done intuitively. In well separable cases, comparing the results of several runs, a so-called elbow rule can be used. This determines the point beyond which, by choosing a larger number of clusters, there is no longer a significant decrease in the cost function (here the cost function is the sum of the distances between the points and the cluster centres)\n",
    "\n",
    "<img src=\"Pics/L11_Elbow.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there are cases where the elbow rule does not apply. For example, when we want to separate a data set with an approximately linear distribution. In this case, again, it is up to us to decide at what level of detail we want to partition the scale.\n",
    "\n",
    "<img src=\"Pics/L11_Size.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: K-Means clastering on pixels\n",
    "\n",
    "Finally, an extra example of using K-means. In the following example, the colours of an image are selected and compressed using the K-Means algorithm. The example is entirely a toy example, in reality there are much better algorithms for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansInitCentroids(X,K):\n",
    "    ################################################ \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ################################################ \n",
    "    return cents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = loadmat(\"bird_small.mat\")\n",
    "A_o = image['A']\n",
    "A = A_o / 255.\n",
    "img_size = A.shape \n",
    "print(img_size)\n",
    "plt.imshow(A)\n",
    "\n",
    "X2 = A.reshape((int(A.size/3),3))\n",
    "\n",
    "K =2\n",
    "max_iters = 5\n",
    "init_cents = kMeansInitCentroids(X2,K)\n",
    "print('Initial Centroids:\\n',init_cents)\n",
    "centroids, idx = runkMeans(X2, init_cents, max_iters)\n",
    "print('K-Means clustering done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Image compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = findClosestCentroids(X2,centroids)\n",
    "\n",
    "X_recovered = centroids[idx.astype(int),:]\n",
    "X_recovered = np.reshape(X_recovered, (A.shape[0], A.shape[1], A.shape[2]))\n",
    "plt.imshow(X_recovered)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
