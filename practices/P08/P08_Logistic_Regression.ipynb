{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pics/logo.png\">\n",
    "\n",
    "Made by **Balázs Nagy**, **Márk Domonkos**\n",
    "\n",
    "[<img src=\"Pics/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_MethodsAndTools/blob/main/practices/P08/P08_Logistic_Regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regresion\n",
    "### Admission to a University:\n",
    "\n",
    "In this exercise, we will use logistic regression to predict the admission of a given student to the university.\n",
    "\n",
    "Suppose that we are university administrators and we want to determine the probability of a given applicant to be admitted successfully based on the results of two admission tests. We have at our disposal the results so far, labelled as to whether the student's application was successful or not.\n",
    "\n",
    "We can use this dataset for our logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Data aquisition (and package imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data3.txt', header = None).to_numpy()          # data aquisition + convert to numpy array\n",
    "X = data[:,0:2]                                                    # data sorting: X\n",
    "m,n = X.shape                                                      # m = num. of samples / n = num. of features\n",
    "Y = data[:,2].reshape(m,1)                                         # data sorting: Y\n",
    "del data                                                           # deleting non-used variables\n",
    "X_original = X.copy()                                              # making a copy of X     \n",
    "\n",
    "print(('X: {}\\nY: {}\\nNum. of samples: {}\\nNum. of features: {}').format(X.shape, Y.shape, m, n))      # print the dimensions and m;n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Plotting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X,Y):\n",
    "    pos=[]                                                  # Admit - 1\n",
    "    neg=[]                                                  # Denied - 0\n",
    "\n",
    "    for i in range(0,Y.size):                               # based on Y vactor we sort the rows of X \n",
    "        if Y[i] ==0:                                        # if Y[row] == 0 then X[row] --> neg; \n",
    "            neg.append(X[i,:])\n",
    "        elif Y[i] ==1:                                      # if Y[row] == 1 then X[row] --> pos; \n",
    "            pos.append(X[i,:])\n",
    "\n",
    "    neg = np.array(neg)                                     # neg to NumPy array\n",
    "    pos = np.array(pos)                                     # pos to NumPy array\n",
    "\n",
    "    plt.scatter(neg[:,0],neg[:,1],marker='x',c=\"r\", label=\"Not admitted\")   # plotting denied with red crosses\n",
    "    plt.scatter(pos[:,0],pos[:,1],marker='o',c=\"g\", label=\"Admitted\")       # plotting admitted with green circles\n",
    "    plt.title(\"Training data\")\n",
    "    plt.xlabel(\"Exam 1 score\")\n",
    "    plt.ylabel(\"Exam 2 score\")\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "\n",
    "    return pos, neg                                          # returning the two array\n",
    "\n",
    "pos,neg=plotData(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can bee seen that the data are with a good estimate (minimal error) linearly (with a straight line) separable.\n",
    "\n",
    "### 3: Data  preparation\n",
    "##### Data normalization:\n",
    "If the data are within an order of magnitude, normalisation is not strictly necessary, but it can be done.\n",
    "\n",
    "(then we add the BIAS so the intersection with the axis will still be in our hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalization(X):\n",
    "#######################################    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "    return X_norm, mean, std                                 \n",
    "\n",
    "X, mean, std = featureNormalization(X_original)            # normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (X.shape[1] == 2):                                   # only add bias when it is needed (don't add bias multiple times)  \n",
    "#######################################                 # don't forget the indentation (because the if)\n",
    "\n",
    "\n",
    "#######################################\n",
    "print(('dim. of X: {}\\nX:\\n{}').format(X.shape, X))     # for checking print the dimensions + X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: The model:\n",
    "\n",
    "Since we are facing a classification problem, our linear regression model will not be fully adequate. Let's look at a simple example. We are trying to decide whether a tumour is lethal or not based on tumour size.\n",
    "\n",
    "<img src=\"Pics/L03_Tumor.png\" width=\"400\">\n",
    "\n",
    "Using the method introduced so far and the\n",
    "\n",
    "$ h_w(x)=XW $ \n",
    "\n",
    "hypothesis is used to fit a line. However, our line exceeds the bounded [0,1] set and is not precise enough. By defining a limit for our fitted line, let it be 0.5, we can decide what our prediction will be.\n",
    "\n",
    "If $h_w(x)\\geq 0.5$, then \"y=1\", this way it is malign. <br>\n",
    "If $h_w(x)< 0.5$, then \"y=0\", this way it is beningn.  <br>\n",
    "\n",
    "However, our prediction may fall below 0 or above 1, which is unnecessary. It would be better to find a bounded hypothesis function that satisfies the following criterion:\n",
    "\n",
    "$0\\leq h_w(x) \\leq 1$\n",
    "\n",
    "Let us introduce the sigmoid function, which satisfies this criterion of being bounded on [0,1].\n",
    "\n",
    "Sigmoid:\n",
    "\n",
    "$ g(z) = \\frac{1}{1+e^{-z}} $\n",
    "\n",
    "<img src= \"Pics/L03_sigmoid.png\" width=450>\n",
    "\n",
    "Using the sigmoid function, we effectively assign a probability to each sample that the tumour size is malign or not. \n",
    "\n",
    "$h_w(x)=P(y=1|X, W)$\n",
    "\n",
    "And we can extend our hypothesis as follows: <br>\n",
    "\n",
    "$ h_w(x) = g(XW) $ <br>\n",
    "\n",
    "where $ g(XW) = \\frac{1}{1+e^{-XW}} $\n",
    "\n",
    "$g(XW)\\geq0.5$ <br>\n",
    "\n",
    "then, if $WX\\geq0$\n",
    "\n",
    "Let's look at two simple examples of classification using a sigmoid function. <br>\n",
    "\n",
    "#### Linear case:\n",
    "\n",
    "$ h_w(x)=g(w_01+w_1x_1+w_2x_2) $\n",
    "\n",
    "$w=[-3\\ 1\\ 1]$ \n",
    "\n",
    "Prediction: $y=1$ if $-3+x_1+x_2\\geq0$\n",
    "\n",
    "$x_1+x_2\\geq3$ \n",
    "\n",
    "<img src= \"Pics/L03_pelda_1.png\" width=200>\n",
    "\n",
    "#### Non-linear case:\n",
    "\n",
    "$ h_w(x)=g(w_01+w_1x_1+w_2x_2+w_3x_1^2+w_4x_2^2) $ \n",
    "\n",
    "$w=[-1\\ 0\\ 0\\ 1\\ 1]$ \n",
    "\n",
    "Prediction: $y=1$ if $-1+x_1^2+x_2^2\\geq0$\n",
    "\n",
    "$x_1^2+x_2^2\\geq1$\n",
    "\n",
    "<img src= \"Pics/L03_pelda_2.png\" width=200>\n",
    "\n",
    "Create the sigmoid function and test the result for -6, 0, 6!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "############################################    \n",
    "\n",
    "\n",
    "############################################    \n",
    "    return g                               \n",
    "\n",
    "print(('Result for input -6: {}\\nResult for input  0: {}\\nResult for input  6: {}').format(sigmoid(-6), sigmoid(0), sigmoid(6)))           # teszt -6 -ra\n",
    "if sigmoid(-6) < 0.01 and sigmoid(0) == 0.5 and sigmoid(6) > 0.99:\n",
    "    print(\"\\n The function works.\")\n",
    "else:\n",
    "    print(\"\\n Something went wrong. Correction is needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Cost function:\n",
    "\n",
    "Since we have changed our hypothesis function, we need to change the cost function we have been using to adapt it to the task. The MSE introduced in our previous examples provided a reasonably smooth convergence, with our modified hypothesis it is not the best choice for solving classification problems as it will result in a non-convex function with many local mininmum points. The question is, can we find a cost function that can define a convex cost function in classification problems? On a convex function, our gradient method is much less likely to get stuck at a local minimum.\n",
    "\n",
    "<img src=\"Pics/L03_Costfunction.png\" width=600>\n",
    "\n",
    "\n",
    "Let:\n",
    "\n",
    "$ C(w) = {-log(h_w(x)),\\   if\\ y=1} $ \n",
    "\n",
    "$ C(w) = {-log(1-h_w(x)),\\ if\\ y=0} $\n",
    "\n",
    "<img src=\"Pics/L03_LogCost.png\" width=300>\n",
    "\n",
    "So we use the following relation as the cost function:\n",
    "\n",
    "$ C(w) = \\frac{1}{m} \\sum{-y^i\\cdot\\log(h_w(x^i))-(1-y^i)\\cdot\\log(1-h_w(x^i))} $\n",
    "\n",
    "\n",
    "### 7: Gradient Descent Algorithm\n",
    "Our aim remains to minimise the cost function. Gradient reduction is still a suitable way for the tuning of the weights by using the derivative of the cost function.\n",
    "\n",
    "$w_j:=w_j-\\mu\\frac{\\partial}{\\partial w_j}C(w)$ \n",
    "\n",
    "The partial derivative of the cost function can be calculated in the usual way:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial w_j}C(w)=\\frac{1}{m}\\sum_{i=1}^{m}(h_w(x^i)-y^i)\\cdot x_j^i $\n",
    "\n",
    "\n",
    "\n",
    "### costFunction definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(w,X,Y):\n",
    "######################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "    return C, grad                                                          \n",
    "\n",
    "initial_w = np.zeros(((n+1),1))                                             # initial weights\n",
    "C1,grad1 = costFunction(initial_w,X,Y)                                      # test 1 with w=[0; 0; 0]\n",
    "\n",
    "print('''Cost and Gradient  at initial weights (zeros):\n",
    "Expected cost (approx.): 0.693\n",
    "Computed:''',C1)\n",
    "print('''Expected gradient(approx.):\n",
    " [[-0.1]\n",
    " [-0.28]\n",
    " [-0.25]]\n",
    "Computed:\\n''',grad1)\n",
    "\n",
    "test_w = np.array([[-24], [13], [16]])                                    # test weights w=[-24;0.2;0.2]\n",
    "C2, grad2 = costFunction(test_w,X,Y)                                      # test 2\n",
    "print('\\nTest weights:',test_w.transpose())\n",
    "print('''Cost and Gradient  at test weights:\n",
    "Expected cost (approx.): 7.74\n",
    "Computed:''',C2)\n",
    "print('''Expected gradient(approx.):\n",
    " [[-0.44]\n",
    " [-0.14]\n",
    " [-0.06]]\n",
    "Computed:\\n''',grad2)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: Gradiens descent method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,Y,w,learning_rate,num_iters):\n",
    "    C_history = []                                      # history of cost function initialization\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################\n",
    "    return w, np.array(C_history)                       # returning with the weights, and the C_history calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0],[0],[0]])     # initial weights\n",
    "epoch = 400\n",
    "\n",
    "w_a, C_history_a = gradientDescent(X,Y,w,0.01,epoch)                           # calculation with lr: 0.01  \n",
    "plt.plot(range(C_history_a.size), C_history_a, label= \"learning r.:0.01\")\n",
    "                                                       \n",
    "w_b, C_history_b = gradientDescent(X,Y,w,0.1, epoch)                             \n",
    "plt.plot(range(C_history_b.size), C_history_b, label= \"learning r.:0.1\")       # calculation with lr: 0.1  \n",
    "\n",
    "w, C_history = gradientDescent(X,Y,w,1,epoch)\n",
    "plt.plot(range(C_history.size), C_history, label= \"learning r.:1\")              # calculation with lr: 1  \n",
    "\n",
    "plt.title(\"Effect of the different constants on the cost function\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost function value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows how the learning rate affected our results:<br>\n",
    "When the learning rate $(\\mu)$ is chosen to be small, convergence is slow. <br>\n",
    "As the learning rate $(\\mu)$ increases, convergence accelerates.<br>\n",
    "\n",
    "(Optional: Try a few more values of learning rate!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''The cost function at found weights by the gradient descent alg.:\n",
    "Expected (approx): 0.203\n",
    "Computed: %.04f''' % C_history[-1])\n",
    "print('''Weights expected (approx.):\n",
    "[1.658 3.883 3.619]\n",
    "Weights computed: \\n''', w.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9: Plot the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pos[:,0],pos[:,1],c=\"g\", marker=\"o\",label=\"Admitted\")     # plotting pos with green circles\n",
    "plt.scatter(neg[:,0],neg[:,1],c=\"r\",marker=\"x\",label=\"Not admitted\")  # plotting neg with red crosses\n",
    "\n",
    "Exam1_val     = np.array([min(X_original[:, 0])-2, max(X_original[:, 0]+2)]) # two points to print the decision boundary\n",
    "Exam1_norm = (Exam1_val - mean[0]) / std[0]\n",
    "Exam2_norm = (-w[0]-w[1]*Exam1_norm)/w[2]                                    # calculate the y coordinates for them\n",
    "Exam2_val     = (Exam2_norm * std[1]) + mean[1]\n",
    "\n",
    "plt.plot(Exam1_val,Exam2_val,\"k\")                                   # plotting the decision boundary\n",
    "plt.title(\"Decision boundary and the training data\")        \n",
    "plt.xlabel(\"Exam 1 score\")\n",
    "plt.ylabel(\"Exam 2 score\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10: Prediction\n",
    "When calculating the prediction, we have to make sure that we perform the same operations on the sample as we did in the data preparation phase before the training. So if we have normalised our data, we must also normalise the new data and add the BIAS. Then we can use our saved weight vector to calculate our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):                                  # prediction function\n",
    "###########################################    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################    \n",
    "    return p, h                                    \n",
    "\n",
    "NewScore = np.array([45,85])\n",
    "pred, h =predict(NewScore)            # results for 45 and 85\n",
    "print('''Expected result of the prediction with [45 , 85] (approx.):\n",
    "Accepted (1) with 0.767 possibility\n",
    "Predicted: %.0f with %.4f possibility''' % (pred, h[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11: Accuracy\n",
    "Let's examine how our linear model performs when evaluating the original data. Calculate the accuracy of the algorithm. To do this, we can use the predict() function we wrote earlier or we can evaluate all the samples in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracy():                                    # accuracy\n",
    "#######################################\n",
    "    \n",
    "    \n",
    "    \n",
    "#######################################\n",
    "    return accuracy                                         \n",
    "\n",
    "print(float(calculateAccuracy()), '% accuracy (approx. 89.0 % expected)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression                       \n",
    "\n",
    "data = pd.read_csv('data3.txt', header = None)       # data aquisition\n",
    "XX = data.iloc[:, 0:2].values.reshape(-1, 2)            # data sorting: X\n",
    "YY = data.iloc[:, 2].values.reshape(-1,)                # data sorting: Y\n",
    "\n",
    "logReg = LogisticRegression().fit(XX,YY)\n",
    "test = np.array([[45, 85]])                             # one sample (1,2) --> [[]]\n",
    "pred = logReg.predict(test)                             # prediction\n",
    "pred_p = logReg.predict_proba(test)                     # test case\n",
    "\n",
    "print(\"\"\"Prediction for the approval:\"\"\",int(pred),\"\"\"\n",
    "The value of the probability:\"\"\",pred_p[0,1])\n",
    "\n",
    "acc = logReg.score(XX,YY)                               # accuracy\n",
    "print('Accuracy on the training data:',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Nonlinear case\n",
    "### Microchip Anomaly:\n",
    "\n",
    "Our task is to determine which of the microchips are defective and which are not based on the measurement results.\n",
    "\n",
    "In this exercise we will work with a non-linearly separable data set. We want to use logistic regression for classification, so we extend the non-linear case by introducing more features (polynomial regression).\n",
    "\n",
    "In this exercise we will test different regularization parameters to better understand how regularization (penalty) works and how it can be used to prevent overfitting. Observe the changes in the decision boundary as the lamdba will be changed. With a small lambda you will notice that there is almost no error in the clustering, but in return you get a very complicated curve. This is not a good decision curve, note that later it accepts (-0.25; 1.5), which seems to be an incorrect decision based on our data set.\n",
    "\n",
    "Using a larger lambda we can see that a simpler decision boundary is created that does not follow the data as closely so it is underfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Make data usable (and package importing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data4.txt', header = None).to_numpy()        # data aquisition then conversion to NumPy array\n",
    "X = data[:,0:2]                                                     # data sorting: X\n",
    "m,n = X.shape                                                       # num. of samples / num. of features\n",
    "Y = data[:,2].reshape(m,1)                                          # data sorting: Y\n",
    "del data                                                            # cleanup\n",
    "\n",
    "print(('X: {}\\nY: {}\\nNum. of samples: {}\\nNum. of features: {}').format(X.shape, Y.shape, m,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sorting the data  (pos/neg) and plotting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X,Y):\n",
    "    pos = []                                                        # good elements\n",
    "    neg = []                                                        # bad elements\n",
    "\n",
    "    for i in range(0,Y.size):                                       # checking Y rows\n",
    "        if Y[i]==1:                                                 # if Y[row]==1 the X[row]-->pos\n",
    "            pos.append(X[i,:])\n",
    "        elif Y[i]==0:                                               # if Y[row]==0 the X[row]-->neg\n",
    "            neg.append(X[i,:])\n",
    "\n",
    "    pos = np.array(pos)                                             # pos to NumPy array\n",
    "    neg = np.array(neg)                                             # neg to NumPy array\n",
    "\n",
    "    plt.scatter(pos[:, 0], pos[:, 1], c=\"g\", marker=\"o\", label=\"OK\")        # plotting pos\n",
    "    plt.scatter(neg[:, 0], neg[:, 1], c=\"r\", marker=\"x\", label=\"Not OK\")    # plotting neg\n",
    "    plt.title(\"Training data\")\n",
    "    plt.xlabel(\"Test 1\")\n",
    "    plt.ylabel(\"Test 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return pos,neg                                                  \n",
    "\n",
    "pos, neg = plotData(X,Y)                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our data cannot be linearly separated.\n",
    "\n",
    "Since our data fall within the interval $[-1, 1]$ for both variables with a fairly good distribution, the data set does not require further normalization.\n",
    "\n",
    "### 3. Fitting\n",
    "\n",
    "In terms of model fittness for the samples, we can distinguish 3 different cases.\n",
    "\n",
    "- Underfit (High Bias) is when the model is too simple and therefore causes a large error on both the training data and the test data.\n",
    "\n",
    "- It is a good fit when we get low error on both the training data and the test data. This means that the learning process has succeeded in learning the relevant information, which allows the new patterns to be classified well enough.\n",
    "\n",
    "- We speak of overfitting (High Variance) if the model has learned the training patterns specifically during the training. This results in a very low error rate during learning, but a high error rate when classifying samples not included in the training data set. \n",
    "\n",
    "<img src=\"Pics/L04_Fittings.png\" width=\"800\">\n",
    "\n",
    "The Underfit and Overfit phenomena during a typical learning run. The goal would be to stop learning at the point where the validation error is the smallest.\n",
    "\n",
    "<img src=\"Pics/L04_BiasVariance.png\" width=\"400\">\n",
    "\n",
    "In practice, we divide our available data into 3 groups (if we have a large enough sample size).\n",
    "- Training set (~70%): data used during training to set the weights of the moddell\n",
    "- Validation set (~15%): Stop learning at the appropriate epoch, hyperparameter optimization\n",
    "- Test set (~15%): Test on independent data, define metrics\n",
    "\n",
    "In this example, we are working with a small data set, so we will not split the data in this way. The theoretical insight only will help our understanding.\n",
    "\n",
    "### Methods to deal with Underfit and Overfit cases:\n",
    "\n",
    "In the case of underfit, our model is too simple. Appropriate compensation could be:\n",
    "- Increasing the number of variables (features)\n",
    "- Choosing a more complex model\n",
    "\n",
    "In case of overfit, our model learns too specifically on the training data. Appropriate compensation could be:\n",
    "- Less input variables\n",
    "- Simpler model\n",
    "- Introduce a penalty (regularization)\n",
    "\n",
    "In the case of regularisation, the guiding principle is: Starting from a complex model (e.g.: multiple variables), the algorithm has a number of possibilities, thus the underfit phenomenon is most likely handled. And by extending the cost function, we penalize if the model uses too many variables. In this way, we create the optimal condition for the simplest model to solve the problem.\n",
    "\n",
    "### 4. Extending input\n",
    "\n",
    "In our Microchip test example, the input variables are the two test results. To solve the desired problem, we need more variables. One possible way to do this is to increase the input variables by the powers of our original variables. \n",
    "\n",
    "$x_1,\\ x_2 \\Rightarrow\\ 1,\\ x_1,\\ x_2,\\ x_1^2,\\ x_1x_2,\\ x_2^2,\\ x_1^3,\\ x_1^2x_2,\\ x_1x_2^2,\\ x_2^3$\n",
    "\n",
    "By expanding our variables up to the 3rd power including the BIAS term, we can count 10 variables instead of the initial 2.\n",
    "\n",
    "Let's create the mapFeature() function that performs the above mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(X1,X2,deg=3):                            \n",
    "#######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################            \n",
    "    return out\n",
    "\n",
    "deg = 3\n",
    "X=mapFeature(X[:,0], X[:,1], deg)                           # using the function\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the input variables have been expanded, we can move on to the construction of the cost function.\n",
    "\n",
    "### Cost function and the gradient descent\n",
    "\n",
    "Our activation function will be the sigmoid function, as we have been used to in previous exercises.\n",
    "\n",
    "The cost function will be augmented with a penalty term according to the following formula:\n",
    "\n",
    "$ C(w)=\\frac{1}{2m}\\sum_{i=1}^m(h_w(x^i)-y^i)^2+\\lambda\\sum_{\\color{red}{j=1}}^nw_{\\color{red}{j}}^2 $\n",
    "\n",
    "where\n",
    "$ \\lambda $ penalty rate<br>\n",
    "$ i $ index of samples starting from 1. $i = 1...n$<br>\n",
    "$ j $ index of input variables starting from 0. $\\color{red}{ j = 0...m}$\n",
    "\n",
    "What should be noted is that the BIAS is not penalized, i.e. the weight $x_{\\color{red}{0}} = 1 $ associated with $ \\color{red}{w_0}$ BIAS should not be taken into account when calculating the penalty. \n",
    "\n",
    "Choosing the value of $ \\lambda$ too high may lead to the case of underfit.  \n",
    "\n",
    "Let us consider how the cost function with the regularization term extended will evolve in the multivariate case and how it will fit into the gradient method.\n",
    "\n",
    "$ C(w)=-\\frac{1}{m}\\sum_{i=1}^{m}y^i\\cdot log(h_w(x^i))+(1-y^i)\\cdot log(1-h_w(x^i))+\\frac{\\lambda}{2m}\\sum_{j=1}^nw_j^2 $\n",
    "\n",
    "For ease of derivation, the regularizing term constant $ \\lambda$ is replaced by $ \\frac{ \\lambda}{2m}$.\n",
    "\n",
    "Basic formula for the weight update of the Gradient Descent method:\n",
    "\n",
    "$ w_j = w_j - \\mu \\color{blue}{\\frac{\\partial}{\\partial w_j}C(w)}$\n",
    "\n",
    "When calculating the derivative of the cost function, the BIAS case must be treated separately. Let us examine it for $w_0$ and $w_1$.\n",
    "\n",
    "$ \\color{blue}{\\frac{\\partial}{\\partial w_0}C(w)}=\\frac{1}{m}\\sum_{i=1}^{m}(h_w(x^i)-y^i)\\cdot x_0^i+{\\color{red} 0}$\n",
    "\n",
    "$ \\color{blue}{\\frac{\\partial}{\\partial w_j}C(w)}=\\frac{1}{m}\\sum_{i=1}^{m}(h_w(x^i)-y^i)\\cdot x_j^i+\\frac{\\lambda}{m}w_j $\n",
    "\n",
    "Taking advantage of the similarities, consider how the cost function and the gradient could be calculated in a function using matrix operations.\n",
    "\n",
    "Now lets implement the costFunctionReg() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def costFunctionReg(w,X,Y,Lambda=1):\n",
    "#######################################################    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "    return C,grad\n",
    "\n",
    "init_w = np.zeros((X.shape[1],1))                   # initial weights (0s)\n",
    "C, grad =costFunctionReg(init_w,X,Y)                # cost function / gradient\n",
    "print('Expected cost at initial weight (zeros): 0.693')\n",
    "print('Calculated cost at initial weight (zeros): %.4f' % C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the successful implementation of the cost function and the calculation of the gradients, the weights can be modified.\n",
    "\n",
    "Gradient Descent algorithm weight modification basic formula:\n",
    "\n",
    "$w_j:=w_j-\\mu\\frac{\\partial}{\\partial w_j}C(w)$ \n",
    "\n",
    "Implement the gradientDescent() function. Use costFunctionReg() and save the cost of each epoch within the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,Y,w,learning_rate,num_iters,Lambda):                         \n",
    "####################################################    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################    \n",
    "    return w, C_history\n",
    "\n",
    "learning_rate = 1\n",
    "epoch = 800\n",
    "Lambda = 0.02\n",
    "\n",
    "w, C_history = gradientDescent(X,Y,init_w,learning_rate,epoch,Lambda)      \n",
    "print('\\nRegularized weight:\\n',w)\n",
    "\n",
    "plt.plot(C_history,label = \"C_history\")    \n",
    "plt.title(\"Cost function trough the iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost function value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Visualization of the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pos[:, 0], pos[:, 1], c=\"g\", marker=\"o\", label=\"OK\")            # plotting pos\n",
    "plt.scatter(neg[:, 0], neg[:, 1], c=\"r\", marker=\"x\", label=\"Not OK\")        # plotting neg\n",
    "\n",
    "u_vals = np.linspace(-1,1.,50)                                              # parameter1 for the boundary plotting\n",
    "v_vals = np.linspace(-1,1.,50)                                              # parameter2\n",
    "z=np.zeros((len(u_vals),len(v_vals)))                                       # initialization of the result matrix\n",
    "\n",
    "for i in range(len(u_vals)):                                                # calculation for each point\n",
    "    for j in range(len(v_vals)):\n",
    "        z[i,j] = mapFeature(u_vals[i],v_vals[j],deg) @ w                        \n",
    "\n",
    "plt.contour(u_vals,v_vals,z.transpose(),0)                                  # plotting the decision boundary\n",
    "plt.title(\"Decision boundary and the training data\")\n",
    "plt.xlabel(\"Exam 1 score\")\n",
    "plt.ylabel(\"Exam 2 score\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationPrediction(w,X):\n",
    "    pred = (sigmoid(X @ w) > 0.5)\n",
    "    return ((np.sum(pred==Y)/m)*100)\n",
    "\n",
    "acc=classificationPrediction(w,X)\n",
    "print('\\nAccuracy of the classification:',acc, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why didn't we learned about nerual networks yet? We will discuss it in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> These exercises are using elements from Andrew NG's Machine Learning course.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
