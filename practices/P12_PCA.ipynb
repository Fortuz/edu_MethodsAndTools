{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\">\n",
    "\n",
    "# Principal Component Analysis (PCA)\n",
    "Made by **Abhishek Wasnik** and **Balázs Nagy**\n",
    "\n",
    "source: [Principal Component Analysis from Scratch in Python](https://www.askpython.com/python/examples/principal-component-analysis)\n",
    "\n",
    "[<img src=\"assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_MethodsAndTools/blob/main/practices/P12_PCA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaBxbduz8mME"
   },
   "source": [
    "Principal component analysis or PCA in short is famously known as a dimensionality reduction technique.\n",
    "\n",
    "It has been around since 1901 and still used as a predominant dimensionality reduction method in machine learning and statistics. PCA is an unsupervised statistical method.\n",
    "\n",
    "In this notebook, we will have some intuition about PCA and will implement it by ourselves from scratch using Python and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDZ2M5DW78t9"
   },
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv2Pdpb89BO8"
   },
   "source": [
    "# Why use PCA in the first place?\n",
    "\n",
    "To support the cause of using PCA let’s look at one example.\n",
    "\n",
    "Suppose we have a dataset having two variables and 10 number of data points. If we were asked to visualize the data points, we can do it very easily. The result is very interpretable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = [2, 8, 1, 4, 22, 15, 25, 29, 4, 2]\n",
    "X2 = [3, 6, 2, 6, 18, 16, 20, 23, 6, 4]\n",
    "\n",
    "X_example = np.column_stack((X1, X2))\n",
    "\n",
    "print('Shape of the example dataset: ', X_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "plt.plot(X_example[:,0], X_example[:,1], 'o')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Data Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w7OyACr9VDX"
   },
   "source": [
    "Now if we try to increase the number of variables it gets almost impossible for us to imagine a dimension higher than three-dimensions.\n",
    "\n",
    "This problem we face when analyzing higher-dimensional datasets is what commonly referred to as **“The curse of dimensionality”**. This term was first coined by Richard E. Bellman.\n",
    "\n",
    "Principal Component analysis reduces high dimensional data to lower dimensions while capturing maximum variability of the dataset. Data visualization is the most common application of PCA. PCA is also used to make the training of an algorithm faster by reducing the number of dimensions of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T93wvle29dGD"
   },
   "source": [
    "# Implementation of PCA with python\n",
    "\n",
    "To grasp the maximum intuition from the content given below, we assume you must know a little bit about linear algebra and [matrices](https://www.askpython.com/python/python-matrix-tutorial). If not then we highly encourage you to watch the [Linear algebra series of 3Blue1Brown](https://www.youtube.com/watch?v=fNk_zzaMoSs) on YouTube by Grant Sanderson, to get a refresher of the concepts as it will prove to be very beneficial in your Machine Learning journey ahead.\n",
    "\n",
    "We can think of Principal Component analysis to be like fitting an n-dimensional ellipsoid to the data so that each axis of the ellipsoid represents a principal component. The larger the principal component axis the larger the variability in data it represents.\n",
    "\n",
    "<img src=\"assets/P12_pca.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWq71sKa92DF"
   },
   "source": [
    "# Steps to implement PCA in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FqFbm5S98zO"
   },
   "source": [
    "## 1. Normalization\n",
    "Subtract the mean of each variable from the dataset so that the dataset should be centered on the origin. Doing this proves to be very helpful when calculating the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPmQ7-XAA7OF"
   },
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    ##### EDIT #####\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ################\n",
    "    return X_norm, mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "X_norm_ex, _, _ = normalize(X_example)\n",
    "print(X_norm_ex)\n",
    "\n",
    "print('Shape of the matrix: ', X_norm_ex.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4h2bBGUc-NMz"
   },
   "source": [
    "Data generated by the above code have dimensions (20,5) i.e. 20 examples and 5 variables for each example. we calculated the mean of each variable and subtracted that from every row of the respective column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9qDu7LF-QsA"
   },
   "source": [
    "## 2. Calculate the Covariance Matrix\n",
    "\n",
    "Calculate the Covariance Matrix of the mean-centered data. You can know more about the covariance matrix in this really informative Wikipedia article [here](https://en.wikipedia.org/wiki/Covariance_matrix).\n",
    "\n",
    "The covariance matrix is a square matrix denoting the covariance of the elements with each other. The covariance of an element with itself is nothing but just its Variance.\n",
    "\n",
    "That’s why the diagonal elements of a covariance matrix are just the variance of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rC1QHfMp8Dil"
   },
   "outputs": [],
   "source": [
    "# calculating the covariance matrix of the mean-centered data.\n",
    "def calc_covMat(X_norm):\n",
    "    ##### EDIT #####\n",
    "    \n",
    "    \n",
    "    ################\n",
    "    return cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "cov_mat_ex = calc_covMat(X_norm_ex)\n",
    "print(cov_mat_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6LVITD5-dt2"
   },
   "source": [
    "We can find easily calculate covariance Matrix using numpy.cov( ) method. The default value for rowvar is set to True, remember to set it to False to get the covariance matrix in the required dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "- What does the covariance matrix shows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UEgFqzZ-fCH"
   },
   "source": [
    "## 3. Compute the Eigenvalues and Eigenvectors\n",
    "\n",
    "Now, compute the Eigenvalues and Eigenvectors for the calculated Covariance matrix. The Eigenvectors of the Covariance matrix we get are Orthogonal to each other and each vector represents a principal axis.\n",
    "\n",
    "A Higher Eigenvalue corresponds to a higher variability. Hence the principal axis with the higher Eigenvalue will be an axis capturing higher variability in the data.\n",
    "\n",
    "Orthogonal means the vectors are mutually perpendicular to each other. Eigenvalues and vectors seem to be very scary until we get the idea and concepts behind it.\n",
    "\n",
    "**HINT**:\n",
    "NumPy `linalg.eigh( )` method returns the eigenvalues and eigenvectors of a complex Hermitian or a real symmetric matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_r0DBBD8FIk"
   },
   "outputs": [],
   "source": [
    "#Calculating Eigenvalues and Eigenvectors of the covariance matrix\n",
    "def calc_eigen(cov_mat):\n",
    "    ##### EDIT #####\n",
    "   \n",
    "\n",
    "    ################\n",
    "    return eigen_values , eigen_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "eigen_values_ex , eigen_vectors_ex = calc_eigen(cov_mat_ex)\n",
    "\n",
    "print(eigen_values_ex)\n",
    "print(eigen_vectors_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJDpDF0B-yCm"
   },
   "source": [
    "## 4. Sort Eigenvalues in descending order\n",
    "\n",
    "Sort the Eigenvalues in the descending order along with their corresponding Eigenvector.\n",
    "\n",
    "Remember each column in the Eigen vector-matrix corresponds to a principal component, so arranging them in descending order of their Eigenvalue will automatically arrange the principal component in descending order of their variability.\n",
    "\n",
    "Hence the first column in our rearranged Eigen vector-matrix will be a principal component that captures the highest variability.\n",
    "\n",
    "**HINT**:\n",
    "`np.argsort` returns an array of indices of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NoSNm278HMU"
   },
   "outputs": [],
   "source": [
    "#sort the eigenvalues in descending order\n",
    "def sort_eigen(eigen_values , eigen_vectors):\n",
    "    ##### EDIT #####\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ################\n",
    "    return sorted_eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "sorted_eigenvectors_ex = sort_eigen(eigen_values_ex , eigen_vectors_ex)\n",
    "\n",
    "print(sorted_eigenvectors_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGCHPv4U--y3"
   },
   "source": [
    "## 5. Select a subset from the rearranged Eigenvalue matrix\n",
    "\n",
    "Select a subset from the rearranged Eigenvalue matrix as per our need i.e. `number_comp = 2`. This means we selected the first two principal components.\n",
    "\n",
    "`n_components = 2` means our final data should be reduced to just 2 variables. if we change it to 3 then we get our data reduced to 3 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJE0VRPN8IgU"
   },
   "outputs": [],
   "source": [
    "# select the first n eigenvectors, n is desired dimension\n",
    "# of our final reduced data.\n",
    "\n",
    "def select_eigensubset(sorted_eigenvectors, n_components = 2):\n",
    "    ##### EDIT #####\n",
    "    \n",
    "    \n",
    "    \n",
    "    ################\n",
    "    return eigenvector_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "eigenvector_subset_ex = select_eigensubset(sorted_eigenvectors_ex, n_components = 1)\n",
    "\n",
    "print(eigenvector_subset_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOlPlQi-_PRg"
   },
   "source": [
    "## 6. Transform the data\n",
    "\n",
    "Finally, transform the data by having a dot product between the Transpose of the Eigenvector subset and the Transpose of the normalized data. By transposing the outcome of the dot product, the result we get is the data reduced to lower dimensions from higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liyYVGOp8Kak"
   },
   "outputs": [],
   "source": [
    "#Transform the data\n",
    "def reduce_dimension(X_norm, eigenvector_subset):\n",
    "    ##### EDIT #####\n",
    "\n",
    "    \n",
    "    \n",
    "    ################\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "X_reduced_ex = reduce_dimension(X_norm_ex, eigenvector_subset_ex)\n",
    "\n",
    "print(X_reduced_ex.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0vJj9ku_T_Q"
   },
   "source": [
    "The final dimensions of X_reduced will be ( 20, 2 ) and originally the data was of higher dimensions ( 20, 5 )\n",
    "\n",
    "Now we can visualize our data with the available tools we have. Hurray! Mission accomplished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0aqVATf_c3D"
   },
   "source": [
    "# Complete Code for Principal Component Analysis in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6rVP6O18MOE"
   },
   "outputs": [],
   "source": [
    "def PCA(X , num_components):\n",
    "    ##### EDIT #####\n",
    "    \n",
    "    #Step-1\n",
    "    \n",
    "\n",
    "    #Step-2\n",
    "    \n",
    "\n",
    "    #Step-3\n",
    "    \n",
    "\n",
    "    #Step-4\n",
    "    \n",
    "\n",
    "    #Step-5\n",
    "    \n",
    "\n",
    "    #Step-6\n",
    "    \n",
    "    \n",
    "    ################\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jx9a19my_hxc"
   },
   "source": [
    "We defined a function named PCA accepting data matrix and the number of components as input arguments.\n",
    "\n",
    "We’ll use [IRIS dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data) and apply our PCA function to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1mxL49S8OkE"
   },
   "outputs": [],
   "source": [
    "#Get the IRIS dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "data = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])\n",
    "\n",
    "#prepare the data\n",
    "x = data.iloc[:,0:4]\n",
    "\n",
    "#prepare the target\n",
    "target = data.iloc[:,4]\n",
    "\n",
    "#Applying it to PCA function\n",
    "##### EDIT #####\n",
    "mat_reduced = ...\n",
    "##### EDIT #####\n",
    "\n",
    "#Creating a Pandas DataFrame of reduced Dataset\n",
    "principal_df = pd.DataFrame(mat_reduced , columns = ['PC1','PC2'])\n",
    "\n",
    "#Concat it with target variable to create a complete Dataset\n",
    "principal_df = pd.concat([principal_df , pd.DataFrame(target)] , axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2NVLcah_zBI"
   },
   "source": [
    "**Important Tip**: we should [standardize data](https://www.askpython.com/python/examples/standardize-data-in-python) wherever necessary before applying any ML algorithm to it. In the above code, we did not standardize our data, but we did so while implementing PCA.\n",
    "\n",
    "Let’s plot our results using the [seaborn](https://www.askpython.com/python-modules/python-seaborn-tutorial) and [matplotlib](https://www.askpython.com/python-modules/matplotlib/python-matplotlib) libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "Idsex_pU8Qyl",
    "outputId": "76e1bc39-288a-4e8e-e91b-7ac0ab7dabd2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "sb.scatterplot(data = principal_df , x = 'PC1',y = 'PC2' , hue = 'target' , s = 60 , palette= 'icefire')\n",
    "plt.title('Reduced Dimension Plot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
